# Backend logic for ì•Œë°”Â·ë‹¨ê¸°ê³„ì•½ ë¦¬ìŠ¤í¬ í•˜ì´ë¼ì´í„°

import os
from dotenv import load_dotenv
from langchain_community.document_loaders import DirectoryLoader, PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI
from langchain_community.vectorstores import Chroma
from PIL import Image
import google.generativeai as genai
import pypdf
import io

# Load environment variables
load_dotenv()


def build_vector_db():
    """
    PDF íŒŒì¼ë“¤ì„ ë¡œë“œí•˜ê³  ë²¡í„° DB(ChromaDB)ë¥¼ êµ¬ì¶•í•˜ëŠ” í•¨ìˆ˜
    """
    print("ë²¡í„° DB êµ¬ì¶•ì„ ì‹œì‘í•©ë‹ˆë‹¤...")

    # 1. DirectoryLoaderë¡œ data/ í´ë”ì˜ ëª¨ë“  .pdf íŒŒì¼ì„ ë¡œë“œ
    print("1. PDF íŒŒì¼ ë¡œë”© ì¤‘...")
    loader = DirectoryLoader(
        path="./data",
        glob="*.pdf",
        loader_cls=PyPDFLoader
    )
    documents = loader.load()
    print(f"   - ì´ {len(documents)}ê°œì˜ í˜ì´ì§€ë¥¼ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")

    # 2. RecursiveCharacterTextSplitter ì‚¬ìš©
    print("2. ë¬¸ì„œë¥¼ ì²­í¬ë¡œ ë¶„í•  ì¤‘...")
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000,
        chunk_overlap=200
    )
    chunks = text_splitter.split_documents(documents)
    print(f"   - ì´ {len(chunks)}ê°œì˜ ì²­í¬ë¡œ ë¶„í• í–ˆìŠµë‹ˆë‹¤.")

    # 3. GoogleGenerativeAIEmbeddings ì‚¬ìš©
    print("3. ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™” ì¤‘...")
    google_api_key = os.getenv("GOOGLE_API_KEY")
    if not google_api_key:
        raise ValueError("GOOGLE_API_KEYê°€ .env íŒŒì¼ì— ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

    embeddings = GoogleGenerativeAIEmbeddings(
        model="models/embedding-001",
        google_api_key=google_api_key
    )

    # 4. Chromaë¥¼ ì‚¬ìš©í•˜ì—¬ ë¡œì»¬ ê²½ë¡œ('./chroma_db')ì— ì €ì¥
    print("4. ChromaDBì— ì €ì¥ ì¤‘...")
    vectorstore = Chroma.from_documents(
        documents=chunks,
        embedding=embeddings,
        persist_directory="./chroma_db"
    )

    # 5. ì™„ë£Œ ë©”ì‹œì§€ ì¶œë ¥
    print(f"\nì´ {len(chunks)}ê°œì˜ ë¬¸ì„œ ì¡°ê°(Chunks)ì´ DBì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤!")

    return vectorstore


def search_db(query):
    """
    ì§ˆë¬¸ì„ ë°›ì•„ì„œ ë²¡í„° DBì—ì„œ ìœ ì‚¬í•œ ë¬¸ì„œë¥¼ ê²€ìƒ‰í•˜ëŠ” í•¨ìˆ˜

    Args:
        query (str): ê²€ìƒ‰í•  ì§ˆë¬¸

    Returns:
        str: ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì •ë¦¬í•œ ë¬¸ìì—´
    """
    print(f"ì§ˆë¬¸: {query}")
    print("ë²¡í„° DB ê²€ìƒ‰ ì¤‘...")

    # 1. GoogleGenerativeAIEmbeddings ì´ˆê¸°í™”
    google_api_key = os.getenv("GOOGLE_API_KEY")
    if not google_api_key:
        raise ValueError("GOOGLE_API_KEYê°€ .env íŒŒì¼ì— ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

    embeddings = GoogleGenerativeAIEmbeddings(
        model="models/embedding-001",
        google_api_key=google_api_key
    )

    # 2. Chroma ë¡œë“œ
    vectorstore = Chroma(
        persist_directory="./chroma_db",
        embedding_function=embeddings
    )

    # 3. ìœ ì‚¬í•œ ë¬¸ì„œ 3ê°œ ê²€ìƒ‰
    results = vectorstore.similarity_search(query, k=3)

    # 4. ê²°ê³¼ ì •ë¦¬
    if not results:
        return "ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

    output = []
    for i, doc in enumerate(results, 1):
        output.append(f"\n=== ë¬¸ì„œ {i} ===")
        output.append(f"ë‚´ìš©: {doc.page_content[:200]}...")  # ì²˜ìŒ 200ìë§Œ í‘œì‹œ
        output.append(f"ì¶œì²˜: {doc.metadata}")

    return "\n".join(output)


def get_answer(query):
    """
    ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ë°›ì•„ì„œ ë²¡í„° DB ê²€ìƒ‰ í›„ Geminië¡œ ë‹µë³€ì„ ìƒì„±í•˜ëŠ” í•¨ìˆ˜

    Args:
        query (str): ì‚¬ìš©ìì˜ ì§ˆë¬¸

    Returns:
        str: Geminiê°€ ìƒì„±í•œ ë‹µë³€
    """
    print(f"\nì§ˆë¬¸: {query}")

    # 1. search_dbë¥¼ í˜¸ì¶œí•´ì„œ ê´€ë ¨ ë¬¸ì„œ ê°€ì ¸ì˜¤ê¸°
    print("ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰ ì¤‘...")
    context = search_db(query)

    # 2. ChatGoogleGenerativeAI ì´ˆê¸°í™”
    google_api_key = os.getenv("GOOGLE_API_KEY")
    if not google_api_key:
        raise ValueError("GOOGLE_API_KEYê°€ .env íŒŒì¼ì— ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

    llm = ChatGoogleGenerativeAI(
        model="gemini-2.5-pro",
        google_api_key=google_api_key,
        temperature=0.7
    )

    # 3. í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì‘ì„±
    prompt = f"""ë‹¹ì‹ ì€ ì‚¬íšŒì´ˆë…„ìƒì„ ìœ„í•œ ì¹œì ˆí•œ ë²•ë¥  ë©˜í†  'í•˜ì´ë¼ì´í„°'ì…ë‹ˆë‹¤.
ì•„ë˜ì˜ [ì°¸ê³  ë¬¸ì„œ]ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëª…ì¾Œí•˜ê²Œ ë‹µí•´ì£¼ì„¸ìš”.

[ë‹µë³€ í•„ìˆ˜ í¬í•¨ í•­ëª©]
1. ğŸš¨ ì›Œë‹ ì‚¬ì¸ (ìœ„ë²• ì—¬ë¶€ íŒë‹¨)
2. ğŸ’¬ ëŒ€ì²˜ ìŠ¤í¬ë¦½íŠ¸ (ì‚¬ì¥ë‹˜ê»˜ ë³´ë‚¼ ì¹´í†¡ ë§íˆ¬ë¡œ)
3. âš–ï¸ ë²•ì  ê·¼ê±° (ì°¸ê³  ë¬¸ì„œì˜ ì¶œì²˜ í™œìš©)

[ì°¸ê³  ë¬¸ì„œ]: {context}
[ì§ˆë¬¸]: {query}
"""

    # 4. Geminië¡œ ë‹µë³€ ìƒì„±
    print("Geminiê°€ ë‹µë³€ì„ ìƒì„± ì¤‘...")
    response = llm.invoke(prompt)

    return response.content


def analyze_contract(file_obj):
    """
    ê³„ì•½ì„œ íŒŒì¼(ì´ë¯¸ì§€ ë˜ëŠ” PDF)ì„ ë¶„ì„í•˜ëŠ” í•¨ìˆ˜

    Args:
        file_obj: Streamlitì˜ st.file_uploaderê°€ ë°˜í™˜í•˜ëŠ” íŒŒì¼ ê°ì²´

    Returns:
        str: ê³„ì•½ì„œ ë¶„ì„ ê²°ê³¼
    """
    print(f"\nê³„ì•½ì„œ ë¶„ì„ ì‹œì‘: {file_obj.name}")

    # 1. íŒŒì¼ íƒ€ì… í™•ì¸
    file_type = file_obj.type
    print(f"íŒŒì¼ íƒ€ì…: {file_type}")

    extracted_text = ""

    # 2. ì´ë¯¸ì§€ íŒŒì¼ì¸ ê²½ìš° (JPEG, PNG ë“±)
    if file_type.startswith("image/"):
        print("ì´ë¯¸ì§€ íŒŒì¼ ê°ì§€ - Gemini Visionìœ¼ë¡œ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì¤‘...")

        # Google API Key í™•ì¸
        google_api_key = os.getenv("GOOGLE_API_KEY")
        if not google_api_key:
            raise ValueError("GOOGLE_API_KEYê°€ .env íŒŒì¼ì— ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

        # Gemini API ì„¤ì •
        genai.configure(api_key=google_api_key)

        # PILë¡œ ì´ë¯¸ì§€ ë¡œë“œ
        image = Image.open(file_obj)

        # Gemini 2.5 Pro Vision ëª¨ë¸ ì‚¬ìš©
        model = genai.GenerativeModel("gemini-2.0-flash-exp")

        # ì´ë¯¸ì§€ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ í”„ë¡¬í”„íŠ¸
        prompt = """ì´ ì´ë¯¸ì§€ëŠ” ê³„ì•½ì„œì…ë‹ˆë‹¤.
ì´ë¯¸ì§€ì— ìˆëŠ” ëª¨ë“  í…ìŠ¤íŠ¸ë¥¼ ì •í™•í•˜ê²Œ ì¶”ì¶œí•´ì£¼ì„¸ìš”.
ê³„ì•½ ë‚´ìš©, ì¡°í•­, ë‚ ì§œ, ì„œëª…ë€ ë“± ëª¨ë“  í…ìŠ¤íŠ¸ë¥¼ ë¹ ì§ì—†ì´ ì¶”ì¶œí•´ì£¼ì„¸ìš”."""

        # Gemini Vision API í˜¸ì¶œ
        response = model.generate_content([prompt, image])
        extracted_text = response.text
        print(f"ì¶”ì¶œëœ í…ìŠ¤íŠ¸ ê¸¸ì´: {len(extracted_text)} ê¸€ì")

    # 3. PDF íŒŒì¼ì¸ ê²½ìš°
    elif file_type == "application/pdf":
        print("PDF íŒŒì¼ ê°ì§€ - pypdfë¡œ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì¤‘...")

        # íŒŒì¼ ê°ì²´ë¥¼ ë°”ì´íŠ¸ë¡œ ì½ê¸°
        pdf_bytes = file_obj.read()
        pdf_file = io.BytesIO(pdf_bytes)

        # pypdfë¡œ PDF ì½ê¸°
        pdf_reader = pypdf.PdfReader(pdf_file)

        # ëª¨ë“  í˜ì´ì§€ì˜ í…ìŠ¤íŠ¸ ì¶”ì¶œ
        for page_num, page in enumerate(pdf_reader.pages, 1):
            page_text = page.extract_text()
            extracted_text += page_text + "\n"
            print(f"í˜ì´ì§€ {page_num} ì¶”ì¶œ ì™„ë£Œ")

        print(f"ì´ ì¶”ì¶œëœ í…ìŠ¤íŠ¸ ê¸¸ì´: {len(extracted_text)} ê¸€ì")

    else:
        return f"ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹ì…ë‹ˆë‹¤: {file_type}\nì§€ì› í˜•ì‹: ì´ë¯¸ì§€(JPG, PNG), PDF"

    # 4. ì¶”ì¶œëœ í…ìŠ¤íŠ¸ê°€ ì—†ìœ¼ë©´ ì˜¤ë¥˜ ë°˜í™˜
    if not extracted_text.strip():
        return "í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. íŒŒì¼ì´ ë¹„ì–´ìˆê±°ë‚˜ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

    # 5. get_answer() í•¨ìˆ˜ë¡œ ê³„ì•½ì„œ ë¶„ì„
    print("\nê³„ì•½ì„œ ë‚´ìš©ì„ ë¶„ì„ ì¤‘...")
    analysis_query = f"""ë‹¤ìŒì€ ê³„ì•½ì„œì˜ ë‚´ìš©ì…ë‹ˆë‹¤. ì´ ê³„ì•½ì„œë¥¼ ë¶„ì„í•´ì„œ ì‚¬íšŒì´ˆë…„ìƒì´ ì£¼ì˜í•´ì•¼ í•  ìœ„í—˜ ìš”ì†Œë¥¼ ì°¾ì•„ì£¼ì„¸ìš”.

[ê³„ì•½ì„œ ë‚´ìš©]
{extracted_text}

ìœ„ ê³„ì•½ì„œì—ì„œ ë¬¸ì œê°€ ë  ìˆ˜ ìˆëŠ” ë¶€ë¶„ì„ ì°¾ì•„ì„œ ë¶„ì„í•´ì£¼ì„¸ìš”."""

    analysis_result = get_answer(analysis_query)

    return analysis_result
